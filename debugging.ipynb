{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging the py files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "input:\n",
    "    - data_file: json data file path\n",
    "    - speech_type: FED or ECB, or both (needs to be implemented)\n",
    "    - task_type: classif or reg, process the data of the two tasks separatively\n",
    "    - val_size: validation set size ratio\n",
    "\n",
    "1. turn each speech into a list of words (for now it's just for summarization, so no need for max_len)\n",
    "2. get rid of strange tokens (needs to be implemented)\n",
    "\n",
    "returns:\n",
    "    - X_train: a list of word_list for training\n",
    "    - X_val: a list of word_list for validation\n",
    "    - y_train: label (classification label or regression price) for training\n",
    "    - y_val: label (classification label or regression price) for validation\n",
    "\n",
    "'''\n",
    "def read_data(data_file, speech_type=['ECB', 'FED'], task_type='classif', val_size=0.2):\n",
    "    with open(data_file, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    \n",
    "    #data = json.loads(data)\n",
    "    \n",
    "    spch_list = []\n",
    "    label_list = []\n",
    "    for data_dict in data:\n",
    "        label_list.append(data_dict['target_' + task_type])\n",
    "        \n",
    "        s = data_dict['speech'][-1]\n",
    "        if s[speech_type[0]]:\n",
    "            l = s[speech_type[0]][0].strip()\n",
    "        else:\n",
    "            l = s[speech_type[1]][0].strip()\n",
    "        #words = l.split(' ')\n",
    "        spch_list.append(l)\n",
    "    \n",
    "    print('[Info] Get {} instances from {}'.format(len(spch_list), data_file))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(spch_list, label_list, test_size=val_size, random_state=42)\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "\n",
    "    train_ds = pd.DataFrame(list(zip(X_train, y_train)),\n",
    "               columns =['text', 'label'])\n",
    "    \n",
    "    dev_ds = pd.DataFrame(list(zip(X_dev, y_dev)),\n",
    "               columns =['text', 'label'])\n",
    "    \n",
    "    test_ds = pd.DataFrame(list(zip(X_test, y_test)),\n",
    "               columns =['text', 'label'])\n",
    "\n",
    "    return train_ds, dev_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Get 1254 instances from summary_text.json\n",
      "[Info] Convert training word instances into sequences of word index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Convert validation word instances into sequences of word index.\n",
      "[Info] Dumping the processed data to pickle file data_processed\n",
      "[Info] Finish.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# get training set and validation set\n",
    "X_train, X__val, y_train, y_val = read_data(\n",
    "    'summary_text.json')\n",
    "\n",
    "# Build vocabulary\n",
    "#if opt.vocab:\n",
    "#    predefined_data = torch.load(opt.vocab)\n",
    "#    assert 'dict' in predefined_data\n",
    "\n",
    "#    print('[Info] Pre-defined vocabulary found.')\n",
    "#    src_word2idx = predefined_data['dict']['src']\n",
    "#    tgt_word2idx = predefined_data['dict']['tgt']\n",
    "#else:\n",
    "\n",
    "\n",
    "\n",
    "# word to index\n",
    "print('[Info] Convert training word instances into sequences of word index.')\n",
    "\n",
    "X_train_insts = [tokenizer(i, return_tensors='pt') for i in X_train]\n",
    "\n",
    "print('[Info] Convert validation word instances into sequences of word index.')\n",
    "X_val_insts = [tokenizer(i, return_tensors='pt') for i in X__val]\n",
    "\n",
    "data = {\n",
    "    #'settings': opt,\n",
    "    'train': {\n",
    "        'X': X_train_insts,\n",
    "        'y': y_train},\n",
    "    'valid': {\n",
    "        'X': X_val_insts,\n",
    "        'y': y_val}}\n",
    "\n",
    "print('[Info] Dumping the processed data to pickle file', 'data_processed')\n",
    "#torch.save(data, opt.save_data)\n",
    "print('[Info] Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_insts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_insts[0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 3930,\n",
       " 4254,\n",
       " 1997,\n",
       " 8332,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 2006,\n",
       " 1996,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 1998,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 12194,\n",
       " 3343,\n",
       " 5656,\n",
       " 1999,\n",
       " 2204,\n",
       " 1998,\n",
       " 2919,\n",
       " 100,\n",
       " 8220,\n",
       " 2013,\n",
       " 1996,\n",
       " 3522,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 1004,\n",
       " 100,\n",
       " 1004,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 1997,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 1998,\n",
       " 100,\n",
       " 100,\n",
       " 2019,\n",
       " 3623,\n",
       " 1999,\n",
       " 14338,\n",
       " 1998,\n",
       " 1037,\n",
       " 9963,\n",
       " 1997,\n",
       " 4722,\n",
       " 5157,\n",
       " 2875,\n",
       " 27143,\n",
       " 2550,\n",
       " 100,\n",
       " 20242,\n",
       " 5816,\n",
       " 3976,\n",
       " 100,\n",
       " 100,\n",
       " 100]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_insts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6041666666666666, 1.0, 0.3076923076923077, 0.5855855855855856, 0.3055555555555556, 1.0, 1.0, 0.2867132867132867, 0.7411764705882353, 0.6304347826086957, 1.0, 1.0, 0.24761904761904763, 0.0, 0.6666666666666666, 0.9, 0.22916666666666666, 1.0, 0.2422680412371134, 0.14545454545454545, 0.3364485981308411, 0.0, 0.4845360824742268, 0.16304347826086957, 0.1282051282051282, 0.22105263157894736, 0.168141592920354, 1.0, 0.3253968253968254, 0.2077922077922078, 1.0, 0.24060150375939848, 1.0, 0.8076923076923077, 0.1711229946524064, 0.2840909090909091, 0.1956521739130435, 0.27586206896551724, 0.18681318681318682, 0.0, 0.21604938271604937, 0.24242424242424243, 0.18548387096774194, 0.0, 0.2677165354330709, 0.14285714285714285, 0.23076923076923078, 0.26277372262773724, 0.2328767123287671, 0.27485380116959063, 1.0, 0.18627450980392157, 1.0, 0.16666666666666666, 1.0, 0.22608695652173913, 0.23076923076923078, 0.28, 0.2782608695652174, 0.5081081081081081, 0.0, 0.2288135593220339, 1.0, 0.26605504587155965, 1.0, 0.0, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.13513513513513514, 0.23140495867768596, 1.0, 0.21311475409836064, 0.8636363636363636, 1.0, 0.20105820105820105, 1.0, 1.0, 0.31932773109243695, 1.0, 0.18110236220472442, 0.19607843137254902, 0.22330097087378642, 0.0, 0.0, 0.18404907975460122, 0.24647887323943662, 0.17647058823529413, 0.21311475409836064, 1.0, 0.40601503759398494, 1.0, 1.0, 0.21686746987951808, 0.18877551020408162, 0.53125, 0.0, 0.23333333333333334, 1.0, 0.19696969696969696, 0.18461538461538463, 0.2682926829268293, 0.7761194029850746, 0.175, 1.0, 0.22627737226277372, 1.0, 0.18181818181818182, 0.2302158273381295, 0.18548387096774194, 0.25, 0.18699186991869918, 0.10077519379844961, 0.0, 0.1826086956521739, 0.415929203539823, 1.0, 0.23214285714285715, 0.15838509316770186, 1.0, 0.24778761061946902, 1.0, 1.0, 0.1917808219178082, 1.0, 0.5526315789473685, 0.7452830188679245, 0.17647058823529413, 0.0, 1.0, 0.1897810218978102, 0.14173228346456693, 0.1509433962264151, 0.1927710843373494, 0.21348314606741572, 0.5743243243243243, 0.2441860465116279, 0.23783783783783785, 0.0, 0.24822695035460993, 0.13861386138613863, 0.24603174603174602, 0.16115702479338842, 0.6907216494845361, 1.0, 0.2153846153846154, 0.1320754716981132, 0.39814814814814814, 0.2824427480916031, 0.11797752808988764, 0.20634920634920634, 0.2421875, 0.19289340101522842, 0.17446808510638298, 1.0, 0.1565217391304348, 0.2988505747126437, 0.20353982300884957, 0.0, 1.0, 1.0, 0.7345132743362832, 0.21428571428571427, 0.2032520325203252, 0.15827338129496402, 1.0, 0.25773195876288657, 0.1292517006802721, 0.0, 1.0, 0.0, 0.42452830188679247, 0.1827956989247312, 1.0, 0.2864583333333333, 0.16822429906542055, 0.13740458015267176, 0.18490566037735848, 0.6029411764705882, 0.16049382716049382, 0.15748031496062992, 0.21511627906976744, 1.0, 0.2111111111111111, 0.20863309352517986, 0.20634920634920634, 1.0, 0.171875, 0.18584070796460178, 0.14736842105263157, 0.1958041958041958, 0.5094339622641509, 0.24528301886792453, 0.14414414414414414, 0.22839506172839505, 0.1941747572815534, 0.2523364485981308, 0.14556962025316456, 0.16551724137931034, 0.20945945945945946, 0.36764705882352944, 0.6911764705882353, 0.37349397590361444, 0.2457627118644068, 0.7096774193548387, 0.18823529411764706, 0.20441988950276244, 0.22772277227722773, 0.0, 1.0, 0.3763440860215054, 1.0, 1.0, 0.0, 1.0, 0.0, 0.14678899082568808, 0.1953125, 0.23478260869565218, 0.15384615384615385, 1.0, 1.0, 1.0, 0.26436781609195403, 0.6956521739130435, 0.23529411764705882, 0.5172413793103449, 1.0, 0.15037593984962405, 0.0, 0.11764705882352941, 0.45901639344262296, 1.0, 0.23214285714285715, 0.2403846153846154, 1.0, 0.20555555555555555, 0.2878787878787879, 1.0, 0.0, 0.1510791366906475, 0.2247191011235955, 0.1981981981981982, 0.27722772277227725, 1.0, 1.0, 0.3958333333333333, 0.17647058823529413, 0.1962025316455696, 1.0, 1.0, 0.15126050420168066, 0.15126050420168066, 0.2080536912751678, 0.31343283582089554, 0.23026315789473684, 0.16161616161616163, 0.20238095238095238, 0.7368421052631579, 1.0, 0.22608695652173913, 0.2361111111111111, 1.0, 0.15789473684210525, 0.25471698113207547, 0.22910216718266255, 0.3238095238095238, 0.2, 0.17318435754189945, 1.0, 0.24705882352941178, 0.6923076923076923, 1.0, 0.14634146341463414, 1.0, 0.0, 0.17073170731707318, 1.0, 0.6610169491525424, 0.26282051282051283, 0.14285714285714285, 0.3411764705882353, 0.2248062015503876, 1.0, 0.32653061224489793, 0.18243243243243243, 0.16778523489932887, 0.17006802721088435, 0.23469387755102042, 1.0, 0.3113207547169811, 0.1875, 0.21621621621621623, 1.0, 0.19827586206896552, 0.7444444444444445, 0.1956521739130435, 0.5290697674418605, 1.0, 0.19576719576719576, 0.2602739726027397, 0.28378378378378377, 0.1891891891891892, 0.18627450980392157, 0.25757575757575757, 1.0, 0.7623762376237624, 0.28865979381443296, 0.2159090909090909, 0.3888888888888889, 0.18888888888888888, 0.18947368421052632, 0.2072072072072072, 0.29213483146067415, 0.19672131147540983, 0.0, 1.0, 1.0, 0.2, 0.18867924528301888, 0.19463087248322147, 0.3115942028985507, 0.0, 0.23469387755102042, 0.2222222222222222, 0.20161290322580644, 0.5555555555555556, 1.0, 0.15555555555555556, 0.2571428571428571, 1.0, 0.18548387096774194, 0.7903225806451613, 0.25, 1.0, 0.3700787401574803, 0.0, 0.0, 0.0, 0.18994413407821228, 1.0, 0.2231404958677686, 0.2708333333333333, 0.20408163265306123, 0.211864406779661, 0.19230769230769232, 0.5409836065573771, 1.0, 0.18840579710144928, 0.18128654970760233, 1.0, 1.0, 0.15178571428571427, 1.0, 0.22535211267605634, 1.0, 0.0, 0.19298245614035087, 1.0, 0.3270440251572327, 0.2598870056497175, 0.21238938053097345, 0.17094017094017094, 0.8028169014084507, 1.0, 0.23333333333333334, 0.13588850174216027, 0.2558139534883721, 0.2265625, 0.19078947368421054, 0.2535211267605634, 0.2085889570552147, 1.0, 0.3333333333333333, 0.4845360824742268, 0.0, 0.22105263157894736, 1.0, 0.0, 0.1926605504587156, 1.0, 0.17391304347826086, 0.0, 1.0, 1.0, 1.0, 0.34265734265734266, 0.2222222222222222, 0.2222222222222222, 0.15976331360946747, 0.18620689655172415, 0.18604651162790697, 0.1590909090909091, 0.0, 0.13963963963963963, 0.0, 1.0, 0.16346153846153846, 0.19047619047619047, 1.0, 1.0, 0.34459459459459457, 0.0, 1.0, 0.27419354838709675, 0.7142857142857143, 0.5316455696202531, 0.19827586206896552, 0.0, 0.2011173184357542, 0.24242424242424243, 0.18699186991869918, 0.0, 0.0, 0.22580645161290322, 0.6212121212121212, 0.5875706214689266, 0.15948275862068967, 0.19047619047619047, 0.208955223880597, 0.18627450980392157, 0.0, 0.0, 0.1619047619047619, 0.205607476635514, 0.297029702970297, 0.19428571428571428, 1.0, 0.14516129032258066, 0.16216216216216217, 0.0, 0.2119205298013245, 0.18781725888324874, 1.0, 0.17982456140350878, 0.25333333333333335, 1.0, 1.0, 0.3246753246753247, 1.0, 0.20168067226890757, 0.24102564102564103, 1.0, 1.0, 0.18085106382978725, 0.2158273381294964, 0.20408163265306123, 1.0, 0.6785714285714286, 0.0, 0.4426229508196721, 0.15714285714285714, 0.175, 0.2222222222222222, 0.5576923076923077, 0.2621951219512195, 0.23943661971830985, 1.0, 0.19540229885057472, 1.0, 1.0, 0.1896551724137931, 1.0, 0.0, 1.0, 0.0, 0.26506024096385544, 1.0, 1.0, 0.0, 0.15609756097560976, 0.24822695035460993, 0.1963470319634703, 0.21875, 0.21666666666666667, 0.21551724137931033, 1.0, 0.1925133689839572, 1.0, 0.23140495867768596, 0.20535714285714285, 0.21428571428571427, 1.0, 0.0, 0.16981132075471697, 0.24166666666666667, 0.7338709677419355, 0.1724137931034483, 0.19753086419753085, 0.19473684210526315, 0.6981132075471698, 0.1885245901639344, 1.0, 0.1702127659574468, 0.0, 0.23255813953488372, 0.74, 0.18085106382978725, 0.19827586206896552, 0.14624505928853754, 1.0, 0.21551724137931033, 0.2805755395683453, 0.2558139534883721, 1.0, 1.0, 0.0, 0.23478260869565218, 0.24475524475524477, 0.18435754189944134, 0.2248062015503876, 0.13157894736842105, 1.0, 0.0, 1.0, 0.13636363636363635, 0.0, 0.0, 0.22916666666666666, 0.21621621621621623, 0.2222222222222222, 0.28125, 0.22142857142857142, 0.18604651162790697, 1.0, 0.6224489795918368, 0.1953125, 1.0, 1.0, 0.0, 0.17894736842105263, 0.1810344827586207, 0.0, 0.17857142857142858, 1.0, 1.0, 0.14423076923076922, 1.0, 1.0, 0.21428571428571427, 1.0, 0.22388059701492538, 1.0, 1.0, 0.0, 0.7, 0.0, 0.17699115044247787, 0.19491525423728814, 1.0, 0.14285714285714285, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.26605504587155965, 1.0, 0.0, 0.17391304347826086, 0.18404907975460122, 0.2403846153846154, 0.0, 0.2916666666666667, 0.14583333333333334, 0.17886178861788618, 0.47530864197530864, 1.0, 0.27419354838709675, 0.2815533980582524, 0.2638888888888889, 0.1564245810055866, 0.24087591240875914, 0.2, 0.29545454545454547, 0.7807017543859649, 0.19166666666666668, 0.25, 0.24770642201834864, 0.559322033898305, 1.0, 0.0, 0.15254237288135594, 0.234375, 0.29133858267716534, 0.18269230769230768, 0.20754716981132076, 0.2748091603053435, 1.0, 0.0, 1.0, 0.0, 0.0, 0.23776223776223776, 0.17829457364341086, 1.0, 0.2627118644067797, 0.2, 0.14049586776859505, 0.24571428571428572, 0.18681318681318682, 0.0, 0.17391304347826086, 0.1761006289308176, 0.17105263157894737, 0.2072072072072072, 0.226890756302521, 1.0, 0.0, 0.0, 0.15151515151515152, 0.22857142857142856, 1.0, 1.0, 0.6842105263157895, 0.2066115702479339, 0.1907514450867052, 0.1853932584269663, 0.0, 0.2913907284768212, 0.23125, 0.19701492537313434, 0.3469387755102041, 0.18018018018018017, 0.23853211009174313, 0.20118343195266272, 0.2564102564102564, 0.4824561403508772, 0.17482517482517482, 1.0, 0.13733905579399142, 0.36904761904761907, 0.14285714285714285, 0.2248062015503876, 1.0, 1.0, 1.0, 0.18518518518518517, 0.22388059701492538, 1.0, 1.0, 1.0, 0.2108843537414966, 0.1792452830188679, 0.5666666666666667, 0.18681318681318682, 0.23728813559322035, 0.2483221476510067, 0.0, 0.2564102564102564, 0.21875, 0.1885245901639344, 1.0, 0.20253164556962025, 0.20652173913043478, 1.0, 0.2097902097902098, 0.41414141414141414, 0.39285714285714285, 1.0, 0.25, 1.0, 1.0, 0.3728813559322034, 0.2826086956521739, 0.15384615384615385, 0.21100917431192662, 1.0, 0.20909090909090908, 1.0, 0.23469387755102042, 1.0, 0.24271844660194175, 1.0, 0.22302158273381295, 1.0, 1.0, 0.14388489208633093, 0.20754716981132076, 0.19402985074626866, 0.2184873949579832, 0.17543859649122806, 0.18518518518518517, 0.16279069767441862, 0.1484375, 0.1694915254237288, 0.3523809523809524, 0.0, 0.0, 1.0, 1.0, 0.20408163265306123, 0.2261904761904762, 0.1896551724137931, 0.16939890710382513, 0.13793103448275862, 0.23333333333333334, 0.0, 0.6484018264840182, 0.25675675675675674, 0.1262135922330097, 0.2268041237113402, 0.21904761904761905, 0.14388489208633093, 0.2289156626506024, 0.0, 0.17204301075268819, 1.0, 0.12, 0.29797979797979796, 0.19196428571428573, 0.0, 1.0, 0.21818181818181817, 0.1619047619047619, 1.0, 0.7966101694915254, 1.0, 0.23076923076923078, 0.19480519480519481, 0.1590909090909091, 0.15, 0.19491525423728814, 0.22826086956521738, 0.6627906976744186, 0.23387096774193547, 0.1885245901639344, 0.26744186046511625, 1.0, 0.1702127659574468, 0.33613445378151263, 0.0, 0.31851851851851853, 1.0, 0.5526315789473685, 0.21153846153846154, 0.0, 1.0, 0.18181818181818182, 0.16463414634146342, 0.22727272727272727, 0.0, 0.16964285714285715, 0.18181818181818182, 1.0, 0.17365269461077845, 0.15328467153284672, 0.1919191919191919, 0.15767634854771784, 0.26666666666666666, 1.0, 0.19135802469135801, 0.3076923076923077, 1.0, 1.0, 0.0, 0.37662337662337664, 0.24210526315789474, 1.0, 1.0, 0.1171875, 1.0, 0.2994350282485876, 1.0, 1.0, 1.0, 1.0, 0.0, 0.29850746268656714, 0.20952380952380953, 0.23931623931623933, 0.1875, 0.3695652173913043, 0.7441860465116279, 0.21238938053097345, 0.1978021978021978, 0.1509433962264151, 0.2702702702702703, 1.0, 0.2222222222222222, 0.13076923076923078, 0.2085889570552147, 0.16806722689075632, 1.0, 0.22727272727272727, 1.0, 1.0, 0.1694915254237288, 0.4482758620689655, 1.0, 0.5303030303030303, 1.0, 1.0, 0.2981366459627329, 0.22885572139303484, 0.0, 0.2375, 0.19101123595505617, 0.1935483870967742, 0.5188679245283019, 0.24793388429752067, 1.0, 1.0, 1.0, 0.24528301886792453, 1.0, 0.20437956204379562, 0.18681318681318682, 0.23809523809523808, 0.0, 0.18269230769230768, 0.2595419847328244, 1.0, 0.2231404958677686, 0.0, 1.0, 1.0, 0.1895910780669145, 0.0, 0.21428571428571427, 0.30701754385964913, 0.1797752808988764, 0.20689655172413793, 0.5546218487394958, 1.0, 0.205607476635514, 0.16666666666666666, 0.1476510067114094, 0.0, 0.3017241379310345, 1.0, 0.21585903083700442, 1.0, 1.0, 0.2215568862275449, 1.0, 0.6417910447761194, 1.0, 0.18994413407821228, 0.1504424778761062, 0.14074074074074075, 0.0, 0.16666666666666666, 1.0, 0.13333333333333333, 0.0, 0.20108695652173914, 0.3401360544217687, 0.25547445255474455, 0.21495327102803738, 0.6936936936936937, 1.0, 0.1864406779661017, 1.0, 0.23770491803278687, 0.0, 0.22935779816513763, 0.1469387755102041, 0.22596153846153846, 0.0, 0.7659574468085106, 0.23931623931623933, 1.0, 1.0, 0.17333333333333334, 0.3883495145631068, 0.21138211382113822, 0.3, 1.0, 0.19078947368421054, 0.24113475177304963, 0.20689655172413793, 0.23243243243243245, 0.0, 0.0, 0.15254237288135594, 1.0, 0.13333333333333333, 0.24489795918367346, 0.67, 0.28431372549019607, 0.16, 0.18, 0.7528089887640449, 1.0, 0.18548387096774194, 0.18487394957983194, 0.0, 0.17964071856287425, 0.21875, 0.2711864406779661, 0.15079365079365079, 1.0, 0.1686746987951807, 0.2288135593220339, 0.23157894736842105, 0.35135135135135137, 1.0, 0.28846153846153844, 0.23333333333333334, 0.19642857142857142, 0.1505016722408027, 0.0, 0.23134328358208955, 0.0, 0.2231404958677686, 0.6666666666666666, 0.3068181818181818, 0.15454545454545454, 1.0, 0.0, 0.7256637168141593, 0.18382352941176472, 0.20625, 1.0, 1.0, 0.2826086956521739, 0.19548872180451127, 0.22941176470588234, 0.2920353982300885, 1.0, 0.13716814159292035, 0.4262295081967213, 0.5576923076923077, 0.0, 1.0, 1.0, 0.22077922077922077, 0.0, 0.27522935779816515, 0.20388349514563106, 0.1574074074074074, 0.2265625, 0.2642857142857143, 0.25833333333333336, 0.5891472868217055, 0.17829457364341086, 0.3176470588235294, 0.22941176470588234, 0.2018348623853211, 0.21656050955414013, 0.336734693877551, 0.18493150684931506, 0.275, 1.0, 1.0, 0.1450381679389313, 0.14705882352941177, 0.14606741573033707, 0.18571428571428572, 1.0, 0.7027027027027027, 0.1346153846153846, 0.2, 0.8928571428571429, 1.0, 1.0, 0.20945945945945946, 0.0, 0.0, 0.17714285714285713, 0.2581818181818182, 0.0, 0.1951219512195122, 1.0, 0.6825396825396826, 0.1743119266055046, 0.0, 0.0, 0.0, 1.0, 0.08947368421052632, 0.76, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.19696969696969696, 0.2518518518518518, 0.0, 1.0, 0.35294117647058826, 0.0, 0.20512820512820512, 0.264, 0.22123893805309736, 0.0, 0.26595744680851063, 0.2235294117647059, 1.0, 0.14785992217898833, 0.21951219512195122, 0.19424460431654678, 0.22, 0.20689655172413793, 0.42452830188679247, 0.0, 0.2231404958677686]\n"
     ]
    }
   ],
   "source": [
    "print([sum([j ==  100 for j in i])/len(i) for i in X_train_insts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tokenizer.convert_ids_to_tokens([100])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer('this is a test', return_tensors='pt')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 3231, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer('This is a test.')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, src_word2idx, tgt_word2idx,\n",
    "        src_insts=None, tgt_insts=None):\n",
    "\n",
    "        assert src_insts\n",
    "        assert not tgt_insts or (len(src_insts) == len(tgt_insts))\n",
    "\n",
    "        src_idx2word = {idx:word for word, idx in src_word2idx.items()} #字典 key:词汇编号 value:词汇\n",
    "        self._src_word2idx = src_word2idx\n",
    "        self._src_idx2word = src_idx2word\n",
    "        self._src_insts = src_insts\n",
    "\n",
    "        tgt_idx2word = {idx:word for word, idx in tgt_word2idx.items()}\n",
    "        self._tgt_word2idx = tgt_word2idx\n",
    "        self._tgt_idx2word = tgt_idx2word\n",
    "        self._tgt_insts = tgt_insts\n",
    "\n",
    "    @property\n",
    "    def n_insts(self):\n",
    "        ''' Property for dataset size '''\n",
    "        return len(self._src_insts) #数据集大小\n",
    "\n",
    "    @property\n",
    "    def src_vocab_size(self):\n",
    "        ''' Property for vocab size '''\n",
    "        return len(self._src_word2idx) #原文词汇集大小\n",
    "\n",
    "    @property\n",
    "    def src_word2idx(self):\n",
    "        ''' Property for word dictionary '''\n",
    "        return self._src_word2idx\n",
    "\n",
    "    @property\n",
    "    def tgt_word2idx(self):\n",
    "        ''' Property for word dictionary '''\n",
    "        return self._tgt_word2idx\n",
    "\n",
    "    @property\n",
    "    def src_idx2word(self):\n",
    "        ''' Property for index dictionary '''\n",
    "        return self._src_idx2word\n",
    "\n",
    "    @property\n",
    "    def tgt_idx2word(self):\n",
    "        ''' Property for index dictionary '''\n",
    "        return self._tgt_idx2word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_insts\n",
    "\n",
    "    def __getitem__(self, idx): #按照index取语句 译文语句可能不存在\n",
    "        if self._tgt_insts:\n",
    "            return self._src_insts[idx], self._tgt_insts[idx]\n",
    "        return self._src_insts[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_name: str, hidden: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "        self.prediction_model = torch.nn.Sequential(\n",
    "                torch.nn.Dropout(dropout),\n",
    "                torch.nn.Linear(self.transformer.config.hidden_size, hidden),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden, 1),\n",
    "            )\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, label: torch.Tensor, *args: torch.Tensor, **kwargs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Feed all arguments to the transformer\n",
    "        transformer_output = self.transformer(*args, **kwargs)\n",
    "        # Get a fixed-size representation with mean pooling\n",
    "        intermediate: torch.Tensor = transformer_output.last_hidden_state.mean(1)\n",
    "        # Apply our MLP\n",
    "        output: torch.Tensor = self.prediction_model(intermediate)[:, 0]\n",
    "        # We feed the output without the sigmoid to the BCEWithLogitsLoss for numerical stability, and only use the prediction for debugging/metric computation\n",
    "        loss: torch.Tensor = self.loss(output, label.to(dtype=torch.float32))\n",
    "        prediction: torch.Tensor = torch.nn.functional.sigmoid(output)\n",
    "        return loss, prediction, transformer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, tokenizer, train_ds, dev_ds, test_ds, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.prepare_data(train=train_ds, dev=dev_ds, test=test_ds)\n",
    "        self.prepare_model()\n",
    "\n",
    "    def prepare_data(self, **kwargs):\n",
    "        def tokenization(batched_text):\n",
    "            return self.tokenizer(batched_text['text'], padding='max_length', truncation=True, max_length=self.tokenizer.model_max_length)\n",
    "\n",
    "        self.dataset = {}\n",
    "        self.iterator = {}\n",
    "        for split in [\"train\", \"dev\", \"test\"]:\n",
    "            data = kwargs[split]\n",
    "            tokenized = data.map(tokenization,\n",
    "                                 batched=True,\n",
    "                                 batch_size=len(data),\n",
    "                                 remove_columns=['text'])\n",
    "            tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "            self.dataset[split] = tokenized\n",
    "            self.iterator[split] = lambda split=split, trainer=self: torch.utils.data.DataLoader(dataset=trainer.dataset[split],\n",
    "                                                                                                 batch_size=trainer.config[\"batch_size\"],\n",
    "                                                                                                 shuffle=True)\n",
    "\n",
    "    def prepare_model(self) -> None:\n",
    "        self.model = Model(self.config[\"model_name\"], self.config[\"hidden\"], self.config[\"dropout\"])\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = transformers.AdamW(self.model.parameters(),\n",
    "                                            lr=self.config[\"learning_rate\"],\n",
    "                                            weight_decay=self.config.get(\"weight_decay\", 0))\n",
    "        max_iterations: int = self.config[\"max_epoch\"]*self.config.get(\"batch_per_epoch\", len(self.dataset[\"train\"]))\n",
    "        # Decrease learning rate linearly\n",
    "        self.scheduler = transformers.get_linear_schedule_with_warmup(self.optimizer, self.config[\"warmup_step\"], max_iterations)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()  # Used to scale gradients to avoid rounding to 0 when using float16\n",
    "\n",
    "    def eval(self, split: str) -> float:\n",
    "        with torch.no_grad():\n",
    "            # Accumulate gradient to better estimate it even with small batches\n",
    "            accuracy_accumulator = 0\n",
    "            num_samples = 0\n",
    "            self.model.eval()\n",
    "            for batch in self.iterator[split]():\n",
    "                batch = {key: value.to(self.device) for key, value in batch.items()}\n",
    "                with torch.cuda.amp.autocast():  # Use mixed-precision float16\n",
    "                    loss, prediction, transformer_output = self.model(**batch)\n",
    "                accuracy_accumulator += ((prediction > 0.5) == batch[\"label\"]).sum().item()\n",
    "                num_samples += batch[\"label\"].shape[0]\n",
    "            accuracy = accuracy_accumulator / num_samples\n",
    "            return accuracy\n",
    "      \n",
    "    def train_step(self) -> None:\n",
    "        \"\"\" Apply the gradients to the parameters. \"\"\"\n",
    "        self.scaler.unscale_(self.optimizer)  # Gradients are scaled in order to avoid rounding to 0 when casting to float16\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config[\"max_grad_norm\"])  # Clip to gradients to a maximum value (to be robust to outliers)\n",
    "        self.scaler.step(self.optimizer)  # Add the gradient to the parameters\n",
    "        self.scheduler.step()  # Update the learning rate\n",
    "        self.scaler.update()  # Update by how much the gradients should be scaled to avoid rounding to 0\n",
    "        self.optimizer.zero_grad()  # Set the gradients to 0 to prepare for the next iteration\n",
    "\n",
    "    def run(self) -> None:\n",
    "        loop = tqdm.trange(self.config[\"max_epoch\"], desc=\"Training\")\n",
    "        best_dev = -math.inf\n",
    "        best_dev_epoch = -math.inf\n",
    "        best_state_dict = None  # Used to save the best model\n",
    "        self.train_acc = []\n",
    "        self.dev_acc = []\n",
    "        for epoch in loop:\n",
    "            # Accumulate gradient to better estimate it even with small batches\n",
    "            accuracy_accumulator = 0\n",
    "            num_samples = 0\n",
    "            self.model.train()\n",
    "            for batch_id, batch in enumerate(self.iterator[\"train\"]()):\n",
    "                if batch_id >= self.config.get(\"batch_per_epoch\", math.inf):\n",
    "                    break\n",
    "                batch = {key: value.to(self.device) for key, value in batch.items()}  \n",
    "                with torch.cuda.amp.autocast():  # Use mixed-precision float16\n",
    "                    loss, prediction, transformer_output = self.model(**batch)\n",
    "                accuracy_accumulator += ((prediction > 0.5) == batch[\"label\"]).sum().item()\n",
    "                num_samples += batch[\"label\"].shape[0]\n",
    "                self.scaler.scale(loss).backward()  # Accumulate the (scaled) gradients\n",
    "                if (1+batch_id) % self.config.get(\"accumulation\", 1) == 0:\n",
    "                    self.train_step()\n",
    "            self.train_step()\n",
    "            accuracy = accuracy_accumulator / num_samples\n",
    "            dev = self.eval(\"dev\")\n",
    "            loop.set_postfix(epoch=f\"{epoch+1:3}\", DEV=f\"{dev:.4f}\", TRAIN=f\"{accuracy:.4f}\")\n",
    "            self.train_acc.append(accuracy)\n",
    "            self.dev_acc.append(dev)\n",
    "            if dev > best_dev:  # If dev score improved\n",
    "                best_dev = dev\n",
    "                best_dev_epoch = epoch\n",
    "                best_state_dict = copy.deepcopy(self.model.state_dict())  # Save the weights of the model\n",
    "            elif epoch - best_dev_epoch > self.config.get(\"patience\", 0):  # If dev score worsened for several steps (or 1 if patience is 0)\n",
    "                break # Early stopping\n",
    "        if best_state_dict is not None:  # If we improved over random initialization\n",
    "            self.model.load_state_dict(best_state_dict)  # Load the model with the best dev score\n",
    "        for split in [\"train\", \"dev\", \"test\"]:\n",
    "            print(f\"Accuracy on {split} split: {self.eval(split)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Get 1254 instances from sums.json\n"
     ]
    }
   ],
   "source": [
    "train_ds_pre_df, dev_ds_pre_df, test_ds_pre_df = read_data(\n",
    "    'sums.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_ds_pre_df['text'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_pre_df.to_csv(\"data_file_pre/train.csv\", index=False)\n",
    "dev_ds_pre_df.to_csv(\"data_file_pre/dev.csv\", index=False)\n",
    "test_ds_pre_df.to_csv(\"data_file_pre/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data_file_pre-f04bcf91d7bc917b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/data_file_pre to /Users/meteor/.cache/huggingface/datasets/csv/data_file_pre-f04bcf91d7bc917b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3192.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 480.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/meteor/.cache/huggingface/datasets/csv/data_file_pre-f04bcf91d7bc917b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 555.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"train.csv\", \"dev\": \"dev.csv\", \"test\": \"test.csv\"}\n",
    "dataset_pre = load_dataset('data_file_pre', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset_pre['train'].shuffle(seed=42)#.select(range(100))\n",
    "dev_ds = dataset_pre['dev'].shuffle(seed=42)#.select(range(100))\n",
    "test_ds = dataset_pre['test'].shuffle(seed=42)#.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.93ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.07ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.61ba/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/meteor/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]/Users/meteor/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/meteor/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Training:   6%|▌         | 3/50 [2:56:19<46:02:32, 3526.65s/it, DEV=0.5124, TRAIN=0.5686, epoch=3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jn/zyqg1tvd3_x4k42nr8p6yr100000gn/T/ipykernel_55056/467647751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jn/zyqg1tvd3_x4k42nr8p6yr100000gn/T/ipykernel_55056/109561696.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use mixed-precision float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0maccuracy_accumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mnum_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jn/zyqg1tvd3_x4k42nr8p6yr100000gn/T/ipykernel_55056/1164257281.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, label, *args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Feed all arguments to the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtransformer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Get a fixed-size representation with mean pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mintermediate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             )\n\u001b[1;32m    330\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bike-ramp/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "trainer_config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"accumulation\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"batch_per_epoch\": 80,\n",
    "    \"max_epoch\": 50,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"hidden\": 256,  # Size of the hidden layer of the MLP.\n",
    "    \"dropout\": 0,  # dropout of the transformers' output, before the MLP.\n",
    "    \"patience\": 5,  # If the dev score worsen 3 epoch in a row, stop the training.\n",
    "    \"warmup_step\": 20,\n",
    "\n",
    "}\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "trainer_config[\"model_name\"] = model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "trainer = Trainer(tokenizer, train_ds, dev_ds, test_ds, trainer_config)\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class PredictDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_file, test_size, max_len, tokenizer):\n",
    "        self.max_len = max_len # use to padding all sequence to a fixed length\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(data_file, 'r') as fp:\n",
    "            data = json.load(fp)\n",
    "        \n",
    "        self.data = []\n",
    "        for data_dict in data:\n",
    "            s = data_dict['speech'][-1]\n",
    "            if s['ECB']:\n",
    "                l = s['ECB'][0].strip()\n",
    "            else:\n",
    "                l = s['FED'][0].strip()\n",
    "            #words = l.split(' ')\n",
    "            text_token = self.tokenizer(l, padding='max_length', truncation=True, max_length=self.max_len)\n",
    "            #print(text_token)\n",
    "\n",
    "            text = torch.tensor(text_token['input_ids'], dtype=float)\n",
    "            text_att = torch.tensor(text_token['attention_mask'], dtype=int)\n",
    "            stock = torch.tensor(data_dict['stock'], dtype=float)\n",
    "            tgt_c = torch.tensor(data_dict['target_classif'], dtype=int)\n",
    "            tgt_r = torch.tensor(data_dict['target_reg'], dtype=float)\n",
    "\n",
    "            \n",
    "            self.data.append({'text_token':text, 'text_att':text_att, 'stock':stock, 'target_classif':tgt_c, 'target_reg':tgt_r})\n",
    "    \n",
    "\n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=test_size,random_state=0)\n",
    "        print(\"=\"*50)\n",
    "        print(\"Data Preprocess Done!\")\n",
    "        print(\"Dataset size:{}, train:{}, val:{}\".\n",
    "              format(len(self.data),len(self.train_data),len(self.test_data)))\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def train_set(self):\n",
    "        '''call this method to switch to train mode'''\n",
    "        self.data = self.train_data\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def test_set(self):\n",
    "        '''call this method to switch to test mode'''\n",
    "        self.data = self.test_data\n",
    "        return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Data Preprocess Done!\n",
      "Dataset size:1254, train:1003, val:251\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import transformers\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "dataset = PredictDataset('sums.json', 0.2, 512, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "808b2d6506002d59e33c894cd39ce65ecb585cca6d5df7e47a6227f88953c98f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bike-ramp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
